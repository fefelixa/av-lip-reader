#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Visual-only lip-reading training with robust label parsing.

Feature layout expected (generated by feature.py):
visual_feats/
  shape/   <base_id>_shape.npy     -> (T, 2)
  dct/     <base_id>_dct.npy       -> (T, N_DCT)
  pca/     <base_id>_pca.npy       -> (T, N_PCA)
  hybrid/  <base_id>_hybrid.npy    -> (T, 2 + N_DCT)

Labels are provided in NAMES.txt (one name per line). File-name prefix is mapped
to labels via a normalization:
  1) take the first token split by "_" or "-"
  2) strip trailing digits
  3) lowercase

So, "Zack009_hybrid.npy" -> "zack", which matches "Zack" in NAMES.txt.
"""

from __future__ import annotations

import argparse
import re
from pathlib import Path
from typing import Dict, List, Tuple

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras import callbacks, layers, models, utils


# Config

PROJECT_ROOT = Path(".")
NAMES_FILE = PROJECT_ROOT / "NAMES.txt"
VISUAL_FEAT_ROOT = PROJECT_ROOT / "visual_feats"
FEATURE_KIND = "pca"  # one of: shape, dct, pca, hybrid
TEST_SIZE = 0.15
VAL_SIZE = 0.15
RANDOM_SEED = 42
EPOCHS = 50
BATCH_SIZE = 32
LEARNING_RATE = 1e-3
PATIENCE = 8  # EarlyStopping


# Utilities

def _normalize_label_from_base_id(base_id: str) -> str:
    """
    Robustly extract a label token from a base_id:
    - take first token by "_" or "-"
    - strip trailing digits
    - lowercase
    """
    token = re.split(r"[_\-]", base_id)[0]
    token = re.sub(r"\d+$", "", token)
    return token.lower()


def load_names(names_file: Path) -> List[str]:
    names: List[str] = []
    with open(names_file, "r", encoding="utf-8") as f:
        for line in f:
            s = line.strip()
            if s:
                names.append(s.lower())
    if not names:
        raise RuntimeError("NAMES.txt is empty.")
    return names


def _pad_truncate_seq(x: np.ndarray, target_T: int) -> np.ndarray:
    """Pad or truncate along time axis (axis=0)."""
    T, D = x.shape
    if T == target_T:
        return x
    if T > target_T:
        return x[:target_T]
    # pad
    pad = np.zeros((target_T - T, D), dtype=x.dtype)
    return np.vstack([x, pad])


def load_visual_dataset(
    feat_root: Path, names_file: Path, kind: str
) -> Tuple[np.ndarray, np.ndarray, List[str]]:
    """
    Load visual features and labels.

    Returns:
        X: (N, T, D, 1)  1-channel for Conv1D-as-Conv2D compatibility
        y: (N, C)        one-hot
        class_names: list[str] in the index order
    """
    kind_dir = feat_root / kind
    if not kind_dir.exists():
        raise RuntimeError(f"Feature directory not found: {kind_dir}")

    names = load_names(names_file)  # lower-cased
    label_to_idx: Dict[str, int] = {n: i for i, n in enumerate(names)}

    files = sorted(kind_dir.glob(f"*_{kind}.npy"))
    if len(files) == 0:
        raise RuntimeError(f"No feature files under {kind_dir}")

    # First pass: collect valid files + label indices
    valid_items: List[Tuple[Path, int]] = []
    for fp in files:
        base = fp.name[: -(len(f"_{kind}.npy"))]  # strip suffix
        norm = _normalize_label_from_base_id(base)
        if norm not in label_to_idx:
            # warn & skip
            print(f"[WARN] label '{norm}' not in NAMES.txt, skip {fp}")
            continue
        valid_items.append((fp, label_to_idx[norm]))

    if not valid_items:
        raise RuntimeError("No valid visual feature files found.")

    # Discover a reasonable T
    lengths = []
    for fp, _ in valid_items[:300]:  # sample up to 200 for speed
        arr = np.load(fp)
        lengths.append(arr.shape[0])
    target_T = int(np.median(lengths))

    X_list, y_list = [], []
    for fp, lid in valid_items:
        arr = np.load(fp)  # (T, D)
        arr = _pad_truncate_seq(arr, target_T)  # (T,D)
        X_list.append(arr)
        y_list.append(lid)

    X = np.stack(X_list, axis=0)  # (N,T,D)
    y_idx = np.array(y_list, dtype=np.int64)

    # Standardize per-feature over N*T samples
    N, T, D = X.shape
    scaler = StandardScaler()
    X_2d = X.reshape(N * T, D)
    X_2d = scaler.fit_transform(X_2d)
    X = X_2d.reshape(N, T, D).astype(np.float32)

    # add channel dim for Conv2D
    X = np.expand_dims(X, axis=-1)  # (N,T,D,1)
    y = utils.to_categorical(y_idx, num_classes=len(names)).astype(np.float32)
    return X, y, names


def build_visual_model(input_shape: Tuple[int, int, int], num_classes: int) -> models.Model:
    """
    A lightweight temporal CNN + BiLSTM classifier.
    Input: (T, D, 1)
    """
    inp = layers.Input(shape=input_shape)

    # Temporal convs (treat D as "width" and T as "height")
    x = layers.Conv2D(32, (5, 1), padding="same", activation="relu")(inp)
    x = layers.BatchNormalization()(x)
    x = layers.MaxPooling2D(pool_size=(2, 1))(x)

    x = layers.Conv2D(64, (5, 1), padding="same", activation="relu")(x)
    x = layers.BatchNormalization()(x)
    x = layers.MaxPooling2D(pool_size=(2, 1))(x)

    # Collapse feature-dim (D) and channel for RNN
    # => (batch, T', F)
    t, d, c = x.shape[1], x.shape[2], x.shape[3]
    x = layers.Reshape((-1, int(d * c)))(x)

    x = layers.Bidirectional(layers.LSTM(128, return_sequences=False))(x)
    x = layers.Dropout(0.3)(x)
    out = layers.Dense(num_classes, activation="softmax")(x)

    model = models.Model(inp, out)
    model.compile(
        optimizer="adam",
        loss="categorical_crossentropy",
        metrics=["accuracy"],
    )
    return model



# Training entry

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--feat_kind", type=str, default=FEATURE_KIND, choices=["shape", "dct", "pca", "hybrid"])
    parser.add_argument("--visual_root", type=str, default=str(VISUAL_FEAT_ROOT))
    parser.add_argument("--names_file", type=str, default=str(NAMES_FILE))
    parser.add_argument("--epochs", type=int, default=EPOCHS)
    parser.add_argument("--batch_size", type=int, default=BATCH_SIZE)
    args = parser.parse_args()

    visual_root = Path(args.visual_root)
    names_file = Path(args.names_file)

    print(f"[INFO] Loading visual dataset: kind={args.feat_kind} root={visual_root}")
    X, y, class_names = load_visual_dataset(visual_root, names_file, args.feat_kind)
    print(f"[INFO] Dataset: X={X.shape}, y={y.shape}, classes={len(class_names)}")

    # Split
    X_train, X_tmp, y_train, y_tmp = train_test_split(
        X, y, test_size=TEST_SIZE + VAL_SIZE, random_state=RANDOM_SEED, stratify=y.argmax(1)
    )
    rel_val = VAL_SIZE / (TEST_SIZE + VAL_SIZE)
    X_val, X_test, y_val, y_test = train_test_split(
        X_tmp, y_tmp, test_size=1 - rel_val, random_state=RANDOM_SEED, stratify=y_tmp.argmax(1)
    )

    # Build
    model = build_visual_model(X.shape[1:], num_classes=y.shape[1])
    model.summary()

    cbs = [
        callbacks.EarlyStopping(monitor="val_accuracy", mode="max", patience=PATIENCE, restore_best_weights=True),
        callbacks.ReduceLROnPlateau(monitor="val_loss", factor=0.5, patience=4, min_lr=1e-5),
    ]

    # Train
    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=args.epochs,
        batch_size=args.batch_size,
        verbose=1,
        callbacks=cbs,
    )

    # Evaluate
    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
    print(f"[RESULT] Visual-only test accuracy = {test_acc:.4f}, loss = {test_loss:.4f}")

    # Save
    out_dir = PROJECT_ROOT / "models"
    out_dir.mkdir(parents=True, exist_ok=True)
    model_path = out_dir / f"visual_only_{args.feat_kind}.keras"
    model.save(model_path)
    print(f"[INFO] Saved model -> {model_path}")


if __name__ == "__main__":
    main()
